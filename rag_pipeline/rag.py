# -*- coding: utf-8 -*-
"""Copy of Copy of rag

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-4AO2Bd4zLcT4swAfjuTsAvxb0uI42dv
"""

!pip install transformers accelerate sentencepiece
!pip install faiss-cpu

from typing import List, Dict, Any, Optional

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from retriever import Retriever

LLM_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"

SYSTEM_PROMPT = """You are a grounded retrieval-augmented assistant for movie and box office questions.

You MUST follow these rules:
1. Answer ONLY from the CONTEXT below. Do not use outside knowledge.
2. DO NOT quote or repeat any lines from the context verbatim (e.g., headings, markdown, instructions).
3. If the CONTEXT does not contain enough information to answer, say:
   "I'm not sure based on the provided context."
4. Do NOT invent information.
5. When you use information from a chunk, cite it in brackets as [Source 1], [Source 2], etc.
6. Be concise but clear.
"""


def format_context(sources: List[Dict[str, Any]]) -> str:
    lines = []
    for i, src in enumerate(sources, start=1):
        meta = src.get("metadata", {}) or {}
        title = meta.get("title", "N/A")
        origin = meta.get("source", "unknown")
        page = meta.get("page", None)

        header = f"[Source {i}] title={title} | source={origin}"
        if page is not None:
            header += f" | page={page}"

        lines.append(header)
        lines.append(src["text"])
        lines.append("")

    return "\n".join(lines).strip()


class RAGPipeline:
    def __init__(self, model_name: str = LLM_MODEL_NAME, device: Optional[str] = None) -> None:
        print("[RAG] Initializing Retriever ...")
        self.retriever = Retriever()

        print(f"[RAG] Loading Phi-3 model: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device

        # NEW: Replace deprecated torch_dtype â†’ dtype
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            dtype=torch.float16 if self.device == "cuda" else torch.float32,
        ).to(self.device)

        print(f"[RAG] Model loaded on {self.device}.")

    def build_prompt(self, question: str, sources: List[Dict[str, Any]]) -> str:
        context_block = format_context(sources)

        prompt = f"""{SYSTEM_PROMPT}

CONTEXT:
{context_block}

QUESTION:
{question}

ANSWER (remember to cite sources like [Source 1], [Source 2]):"""
        return prompt

    def answer(self, question: str, k: int = 5, max_new_tokens: int = 96) -> Dict[str, Any]:
        if k < 3:
            k = 3

        print(f"[RAG] Retrieving top-{k} sources for: {question!r}")
        sources = self.retriever.retrieve(question, k=k)

        if not sources:
            return {
                "answer": "I'm not sure based on the provided context.",
                "sources": [],
            }

        prompt = self.build_prompt(question, sources)

        print("[RAG] Generating answer with Phi-3...")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]

        output_ids = self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=self.tokenizer.eos_token_id,
        )

        generated_ids = output_ids[0, input_ids.shape[-1]:]

        answer_text = self.tokenizer.decode(
            generated_ids, skip_special_tokens=True
        ).strip()

        if "\n\n" in answer_text:
            answer_text = answer_text.split("\n\n", 1)[0].strip()

        if not answer_text:
            answer_text = "I'm not sure based on the provided context."

        return {
            "answer": answer_text,
            "sources": sources,
        }


def main():
    rag = RAGPipeline()

    question1 = "What factors influence box office performance according to the context?"
    result1 = rag.answer(question1, k=5)

    print("\n================= QUESTION 1 =================")
    print("Q:", question1)
    print("\n=== ANSWER ===")
    print(result1["answer"])
    print("\n=== SOURCES USED ===")
    for i, src in enumerate(result1["sources"], start=1):
        meta = src.get("metadata", {})
        title = meta.get("title", "N/A")
        origin = meta.get("source", "unknown")
        page = meta.get("page", None)
        print(f"[Source {i}] title={title} | source={origin} | page={page}")

    question2 = "Which movie in the dataset has the highest IMDb rating?"
    result2 = rag.answer(question2, k=5)

    print("\n================= QUESTION 2 =================")
    print("Q:", question2)
    print("\n=== ANSWER ===")
    print(result2["answer"])
    print("\n=== SOURCES USED ===")
    for i, src in enumerate(result2["sources"], start=1):
        meta = src.get("metadata", {})
        title = meta.get("title", "N/A")
        origin = meta.get("source", "unknown")
        page = meta.get("page", None)
        print(f"[Source {i}] title={title} | source={origin} | page={page}")


if __name__ == "__main__":
    main()